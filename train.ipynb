{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b811f73b-c360-421b-9d0a-b57f76d48c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (78.1.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp313-cp313-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/204.1 MB 10.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 7.6/204.1 MB 24.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 14.4/204.1 MB 27.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 21.2/204.1 MB 29.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 28.6/204.1 MB 30.3 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 35.4/204.1 MB 31.0 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 43.0/204.1 MB 31.8 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.8/204.1 MB 31.3 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 55.6/204.1 MB 31.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 62.9/204.1 MB 31.8 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 70.3/204.1 MB 32.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 76.8/204.1 MB 32.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 84.4/204.1 MB 32.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 91.8/204.1 MB 32.4 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 99.1/204.1 MB 32.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.6/204.1 MB 32.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 111.7/204.1 MB 32.2 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 116.4/204.1 MB 31.7 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 123.5/204.1 MB 31.7 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 130.5/204.1 MB 31.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 137.1/204.1 MB 31.8 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 144.7/204.1 MB 32.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 151.0/204.1 MB 31.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 158.9/204.1 MB 32.1 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 165.7/204.1 MB 32.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 172.0/204.1 MB 32.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 178.8/204.1 MB 32.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 185.1/204.1 MB 31.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 191.4/204.1 MB 31.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 197.4/204.1 MB 31.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.2/204.1 MB 31.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  203.9/204.1 MB 31.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 204.1/204.1 MB 30.1 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 30.2 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 24.1 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 25.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, fsspec, filelock, torch\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89befa7f-bef7-4e52-ad38-bdcd951bf8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting numpy (from torchvision)\n",
      "  Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: torch==2.6.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.6.0)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\marce\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.21.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/1.6 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 23.7 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 28.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.0/12.6 MB 27.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 27.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pillow, numpy, torchvision\n",
      "Successfully installed numpy-2.2.4 pillow-11.2.1 torchvision-0.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8eb1a9d-711a-4152-8bcf-4ba0f72ed13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d5b3f08-ab6b-4634-85d9-732b48f46547",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'dataset'\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.0005\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class_map = {\n",
    "    'closedLeftEyes': 0,\n",
    "    'closedRightEyes': 0,\n",
    "    'openLeftEyes': 1,\n",
    "    'openRightEyes': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2d579ba4-95bb-4a56-a396-372b47ecac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        #We load whole dataset from subfolders and put label on each image\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "        for folder, label in class_map.items():\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            for fname in os.listdir(folder_path):\n",
    "                if fname.endswith('.jpg'):\n",
    "                    self.samples.append((os.path.join(folder_path, fname), label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = default_loader(path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "363a365e-dbad-4832-8a15-65e88187c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "    This is a lightweight CNN designed for binary eye state classification based on small input grayscale images (24x24 px) \n",
    "    of individual eyes. The architecture consists of two convolutional layers with BatchNorm and MaxPooling, followed by \n",
    "    two fully connected layers. Dropout is used to reduce overfitting. \n",
    "'''\n",
    "\n",
    "class EyeBlinkCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EyeBlinkCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 6 * 6)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0c7e2add-f219-4fc5-b45c-ef9a0778d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This are optimal data augmentation for my model\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "179f7409-e887-4f74-b244-f798ba3c2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EyeDataset(DATASET_DIR, transform=train_transforms)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "904e0e60-dfb0-4ba6-af3a-de0ce924155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EyeBlinkCNN().to(DEVICE)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc6b8c9f-8ae8-442a-ba06-aa717f282ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Loss: 0.4029, Accuracy: 81.26%\n",
      "Epoch 2/60, Loss: 0.1857, Accuracy: 92.70%\n",
      "Epoch 3/60, Loss: 0.1417, Accuracy: 94.51%\n",
      "Epoch 4/60, Loss: 0.1196, Accuracy: 95.40%\n",
      "Epoch 5/60, Loss: 0.1149, Accuracy: 95.46%\n",
      "Epoch 6/60, Loss: 0.0986, Accuracy: 96.41%\n",
      "Epoch 7/60, Loss: 0.0970, Accuracy: 96.26%\n",
      "Epoch 8/60, Loss: 0.0996, Accuracy: 96.04%\n",
      "Epoch 9/60, Loss: 0.0924, Accuracy: 96.60%\n",
      "Epoch 10/60, Loss: 0.0848, Accuracy: 97.05%\n",
      "Epoch 11/60, Loss: 0.0837, Accuracy: 96.82%\n",
      "Epoch 12/60, Loss: 0.0827, Accuracy: 96.86%\n",
      "Epoch 13/60, Loss: 0.0782, Accuracy: 97.13%\n",
      "Epoch 14/60, Loss: 0.0726, Accuracy: 97.30%\n",
      "Epoch 15/60, Loss: 0.0724, Accuracy: 97.32%\n",
      "Epoch 16/60, Loss: 0.0705, Accuracy: 97.28%\n",
      "Epoch 17/60, Loss: 0.0690, Accuracy: 97.48%\n",
      "Epoch 18/60, Loss: 0.0605, Accuracy: 97.73%\n",
      "Epoch 19/60, Loss: 0.0591, Accuracy: 97.96%\n",
      "Epoch 20/60, Loss: 0.0663, Accuracy: 97.46%\n",
      "Epoch 21/60, Loss: 0.0589, Accuracy: 97.79%\n",
      "Epoch 22/60, Loss: 0.0579, Accuracy: 97.81%\n",
      "Epoch 23/60, Loss: 0.0563, Accuracy: 97.96%\n",
      "Epoch 24/60, Loss: 0.0586, Accuracy: 97.85%\n",
      "Epoch 25/60, Loss: 0.0507, Accuracy: 98.02%\n",
      "Epoch 26/60, Loss: 0.0460, Accuracy: 98.23%\n",
      "Epoch 27/60, Loss: 0.0491, Accuracy: 98.04%\n",
      "Epoch 28/60, Loss: 0.0443, Accuracy: 98.35%\n",
      "Epoch 29/60, Loss: 0.0455, Accuracy: 98.31%\n",
      "Epoch 30/60, Loss: 0.0409, Accuracy: 98.45%\n",
      "Epoch 31/60, Loss: 0.0506, Accuracy: 98.16%\n",
      "Epoch 32/60, Loss: 0.0530, Accuracy: 98.06%\n",
      "Epoch 33/60, Loss: 0.0468, Accuracy: 98.27%\n",
      "Epoch 34/60, Loss: 0.0510, Accuracy: 98.02%\n",
      "Epoch 35/60, Loss: 0.0389, Accuracy: 98.53%\n",
      "Epoch 36/60, Loss: 0.0365, Accuracy: 98.49%\n",
      "Epoch 37/60, Loss: 0.0483, Accuracy: 98.10%\n",
      "Epoch 38/60, Loss: 0.0297, Accuracy: 98.87%\n",
      "Epoch 39/60, Loss: 0.0428, Accuracy: 98.39%\n",
      "Epoch 40/60, Loss: 0.0436, Accuracy: 98.37%\n",
      "Epoch 41/60, Loss: 0.0401, Accuracy: 98.56%\n",
      "Epoch 42/60, Loss: 0.0362, Accuracy: 98.49%\n",
      "Epoch 43/60, Loss: 0.0337, Accuracy: 98.66%\n",
      "Epoch 44/60, Loss: 0.0320, Accuracy: 98.78%\n",
      "Epoch 45/60, Loss: 0.0412, Accuracy: 98.56%\n",
      "Epoch 46/60, Loss: 0.0315, Accuracy: 98.91%\n",
      "Epoch 47/60, Loss: 0.0349, Accuracy: 98.64%\n",
      "Epoch 48/60, Loss: 0.0319, Accuracy: 98.80%\n",
      "Epoch 49/60, Loss: 0.0301, Accuracy: 98.93%\n",
      "Epoch 50/60, Loss: 0.0264, Accuracy: 99.05%\n",
      "Epoch 51/60, Loss: 0.0290, Accuracy: 98.84%\n",
      "Epoch 52/60, Loss: 0.0316, Accuracy: 98.66%\n",
      "Epoch 53/60, Loss: 0.0248, Accuracy: 99.05%\n",
      "Epoch 54/60, Loss: 0.0323, Accuracy: 98.82%\n",
      "Epoch 55/60, Loss: 0.0246, Accuracy: 99.07%\n",
      "Epoch 56/60, Loss: 0.0262, Accuracy: 98.89%\n",
      "Epoch 57/60, Loss: 0.0262, Accuracy: 98.97%\n",
      "Epoch 58/60, Loss: 0.0209, Accuracy: 99.17%\n",
      "Epoch 59/60, Loss: 0.0252, Accuracy: 99.09%\n",
      "Epoch 60/60, Loss: 0.0224, Accuracy: 99.05%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Standard training loop for a classification using cross-entropy loss and the Adam optimizer.\n",
    "    For each epoch, the model is trained on all batches and accuracy is calculated over the full training set.\n",
    "\n",
    "'''\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "31ae8390-921a-4891-a409-c6a3a1f90cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'eye_blink_cnn.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0ef8f-4dc4-41fd-ae22-5424d8904907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
